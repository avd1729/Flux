{"source": "IMP Q WA.pdf", "page": 1, "text": "1. Understanding Web Analytics \n• \nDefinition: \nMeasurement, collection, analysis, and reporting of internet data for \nunderstanding and optimizing web usage. • \nPurpose: \no Improve website effectiveness. o Conduct market and business research. o Track marketing campaigns and conversion rates. • \nTypes of Data: \nBehavioral – What users do (page views, clicks, navigation paths). Attitudinal – Why they do it (motivation, preferences, satisfaction). • \nBenefits: \no Identifies popular content."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "Attitudinal – Why they do it (motivation, preferences, satisfaction). • \nBenefits: \no Identifies popular content. o Helps in customer segmentation. o Aids in ROI measurement for campaigns. • \nChallenges: \no Accuracy affected by bots, caching, cookie deletion. o Data tied to devices, not individuals. 2. Competitive Intelligence \n• \nDefinition: \nThe process of gathering and analyzing data about competitors’ online \nperformance and market trends."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "2. Competitive Intelligence \n• \nDefinition: \nThe process of gathering and analyzing data about competitors’ online \nperformance and market trends. • \nMethods: \no Panel-based data: From users who install tracking software (e.g., \ncomScore). o ISP-based data: From Internet Service Providers (e.g., Hitwise). o Search engine data: Google Trends, Microsoft adCenter. • \nApplications: \no Identify market share trends."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "o Spot new market opportunities. o Understand competitor traffic sources and strategies. • \nBest Practices: \no Segment competitor data by geography, device, or demographic. o Compare over time, not just in single snapshots. 3. Research Data \n• \nPurpose: Understand the “why” behind user actions (qualitative \ninsight). • \nMethods: \no Surveys: Collect feedback on satisfaction, intent, preferences. o Heuristic evaluations: Expert review based on usability \nprinciples."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "• \nMethods: \no Surveys: Collect feedback on satisfaction, intent, preferences. o Heuristic evaluations: Expert review based on usability \nprinciples. o Usability testing: Observe users completing defined tasks. o Site visits: Observe real-world usage contexts. • \nBenefits: \no Reveals hidden pain points. o Complements clickstream (quantitative) data. • \nOrganizational Note: Best results occur when research and analytics \nteams collaborate. 4."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "• \nOrganizational Note: Best results occur when research and analytics \nteams collaborate. 4. Critical Components \n• \nCustomer Centricity: \no Prioritize user needs over purely business-focused metrics. o Measure satisfaction, task completion, content findability. • \nBusiness Questions Approach: \no Shift from “give me a report” to “help me solve X business \nproblem.” \no Use open-ended, action-oriented questions. • \n10/90 Rule:"}
{"source": "IMP Q WA.pdf", "page": 3, "text": "o Spend 10% on tools, 90% on skilled people who can interpret and \nact on data. • \nIdeal Analyst Traits: \no Tool-agnostic, curious, communicative, and business-minded. • \nOrganizational Ownership: \no Analytics should be business-owned, not purely IT-driven. 5. KPIs & Usability Testing \nKPIs (Key Performance Indicators) \n• \nDefinition: Metrics directly linked to business goals. • \nExamples: \no Commerce: Conversion rate, average order value. o Lead gen: Cost per lead, bounce rate."}
{"source": "IMP Q WA.pdf", "page": 3, "text": "• \nExamples: \no Commerce: Conversion rate, average order value. o Lead gen: Cost per lead, bounce rate. o Support: Task completion, search query trends. • \nSelection Process: \no Define site purpose. o Identify main visitor segments. o Choose metrics that indicate success for each segment. Usability Testing \n• \nPurpose: Evaluate how easy and efficient it is for users to complete \ntasks on a website. • \nProcess: Prepare → Conduct → Analyze → Follow-up."}
{"source": "IMP Q WA.pdf", "page": 3, "text": "Usability Testing \n• \nPurpose: Evaluate how easy and efficient it is for users to complete \ntasks on a website. • \nProcess: Prepare → Conduct → Analyze → Follow-up. • \nBenefits: Detects usability issues early, provides context for analytics \ndata. 6. Heuristic Evaluation \n• \nDefinition: Expert review of a site/application using established \nusability heuristics. • \nPrinciples Used (examples): \no Visibility of system status."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "o Match between system and real world. o User control and freedom. o Error prevention and recovery. • \nAdvantages: \no Quick and cost-effective. o Can be performed early in development. • \nLimitations: \no May miss issues only revealed through actual user interaction. o Best when combined with usability testing. 7. Survey Feedback \n• \nPurpose: Collect attitudinal data directly from users. • \nBest Practices: \no Keep surveys short (1–3 questions) to encourage completion."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "Survey Feedback \n• \nPurpose: Collect attitudinal data directly from users. • \nBest Practices: \no Keep surveys short (1–3 questions) to encourage completion. o Ask only actionable questions (e.g., “What prevented you from \npurchasing today?”). o Use cookie-based controls to avoid over-surveying. • \nDelivery Methods: \no On-page popups after task completion. o Email follow-ups for recent visitors. • \nAdvantages: \no Direct voice-of-customer data."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "o Email follow-ups for recent visitors. • \nAdvantages: \no Direct voice-of-customer data. o Identifies satisfaction levels and improvement areas. 8. Types of Data \n1. Behavioral Data – Quantitative, from logs/tags: \no Page views, clicks, time on page, navigation flow. 2. Attitudinal Data – Qualitative, from user feedback: \no Satisfaction levels, reasons for visits, perceived ease of use."}
{"source": "IMP Q WA.pdf", "page": 5, "text": "3. Outcome Data – Business result-oriented: \no Orders placed, leads generated, support cases resolved. 4. Competitive Data – External benchmarking: \no Market share, traffic rank, competitor keywords. 5. Research Data – Contextual understanding: \no Usability tests, heuristic reviews, in-depth interviews."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "1. Understanding Web Analytics \n• \nDefinition: \nMeasurement, collection, analysis, and reporting of internet data for \nunderstanding and optimizing web usage. • \nPurpose: \no Improve website effectiveness. o Conduct market and business research. o Track marketing campaigns and conversion rates. • \nTypes of Data: \nBehavioral – What users do (page views, clicks, navigation paths). Attitudinal – Why they do it (motivation, preferences, satisfaction). • \nBenefits: \no Identifies popular content."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "Attitudinal – Why they do it (motivation, preferences, satisfaction). • \nBenefits: \no Identifies popular content. o Helps in customer segmentation. o Aids in ROI measurement for campaigns. • \nChallenges: \no Accuracy affected by bots, caching, cookie deletion. o Data tied to devices, not individuals. 2. Competitive Intelligence \n• \nDefinition: \nThe process of gathering and analyzing data about competitors’ online \nperformance and market trends."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "2. Competitive Intelligence \n• \nDefinition: \nThe process of gathering and analyzing data about competitors’ online \nperformance and market trends. • \nMethods: \no Panel-based data: From users who install tracking software (e.g., \ncomScore). o ISP-based data: From Internet Service Providers (e.g., Hitwise). o Search engine data: Google Trends, Microsoft adCenter. • \nApplications: \no Identify market share trends."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "o Spot new market opportunities. o Understand competitor traffic sources and strategies. • \nBest Practices: \no Segment competitor data by geography, device, or demographic. o Compare over time, not just in single snapshots. 3. Research Data \n• \nPurpose: Understand the “why” behind user actions (qualitative \ninsight). • \nMethods: \no Surveys: Collect feedback on satisfaction, intent, preferences. o Heuristic evaluations: Expert review based on usability \nprinciples."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "• \nMethods: \no Surveys: Collect feedback on satisfaction, intent, preferences. o Heuristic evaluations: Expert review based on usability \nprinciples. o Usability testing: Observe users completing defined tasks. o Site visits: Observe real-world usage contexts. • \nBenefits: \no Reveals hidden pain points. o Complements clickstream (quantitative) data. • \nOrganizational Note: Best results occur when research and analytics \nteams collaborate. 4."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "• \nOrganizational Note: Best results occur when research and analytics \nteams collaborate. 4. Critical Components \n• \nCustomer Centricity: \no Prioritize user needs over purely business-focused metrics. o Measure satisfaction, task completion, content findability. • \nBusiness Questions Approach: \no Shift from “give me a report” to “help me solve X business \nproblem.” \no Use open-ended, action-oriented questions. • \n10/90 Rule:"}
{"source": "IMP Q WA.pdf", "page": 3, "text": "o Spend 10% on tools, 90% on skilled people who can interpret and \nact on data. • \nIdeal Analyst Traits: \no Tool-agnostic, curious, communicative, and business-minded. • \nOrganizational Ownership: \no Analytics should be business-owned, not purely IT-driven. 5. KPIs & Usability Testing \nKPIs (Key Performance Indicators) \n• \nDefinition: Metrics directly linked to business goals. • \nExamples: \no Commerce: Conversion rate, average order value. o Lead gen: Cost per lead, bounce rate."}
{"source": "IMP Q WA.pdf", "page": 3, "text": "• \nExamples: \no Commerce: Conversion rate, average order value. o Lead gen: Cost per lead, bounce rate. o Support: Task completion, search query trends. • \nSelection Process: \no Define site purpose. o Identify main visitor segments. o Choose metrics that indicate success for each segment. Usability Testing \n• \nPurpose: Evaluate how easy and efficient it is for users to complete \ntasks on a website. • \nProcess: Prepare → Conduct → Analyze → Follow-up."}
{"source": "IMP Q WA.pdf", "page": 3, "text": "Usability Testing \n• \nPurpose: Evaluate how easy and efficient it is for users to complete \ntasks on a website. • \nProcess: Prepare → Conduct → Analyze → Follow-up. • \nBenefits: Detects usability issues early, provides context for analytics \ndata. 6. Heuristic Evaluation \n• \nDefinition: Expert review of a site/application using established \nusability heuristics. • \nPrinciples Used (examples): \no Visibility of system status."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "o Match between system and real world. o User control and freedom. o Error prevention and recovery. • \nAdvantages: \no Quick and cost-effective. o Can be performed early in development. • \nLimitations: \no May miss issues only revealed through actual user interaction. o Best when combined with usability testing. 7. Survey Feedback \n• \nPurpose: Collect attitudinal data directly from users. • \nBest Practices: \no Keep surveys short (1–3 questions) to encourage completion."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "Survey Feedback \n• \nPurpose: Collect attitudinal data directly from users. • \nBest Practices: \no Keep surveys short (1–3 questions) to encourage completion. o Ask only actionable questions (e.g., “What prevented you from \npurchasing today?”). o Use cookie-based controls to avoid over-surveying. • \nDelivery Methods: \no On-page popups after task completion. o Email follow-ups for recent visitors. • \nAdvantages: \no Direct voice-of-customer data."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "o Email follow-ups for recent visitors. • \nAdvantages: \no Direct voice-of-customer data. o Identifies satisfaction levels and improvement areas. 8. Types of Data \n1. Behavioral Data – Quantitative, from logs/tags: \no Page views, clicks, time on page, navigation flow. 2. Attitudinal Data – Qualitative, from user feedback: \no Satisfaction levels, reasons for visits, perceived ease of use."}
{"source": "IMP Q WA.pdf", "page": 5, "text": "3. Outcome Data – Business result-oriented: \no Orders placed, leads generated, support cases resolved. 4. Competitive Data – External benchmarking: \no Market share, traffic rank, competitor keywords. 5. Research Data – Contextual understanding: \no Usability tests, heuristic reviews, in-depth interviews."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "1. Understanding Web Analytics \n• \nDefinition: \nMeasurement, collection, analysis, and reporting of internet data for \nunderstanding and optimizing web usage. • \nPurpose: \no Improve website effectiveness. o Conduct market and business research. o Track marketing campaigns and conversion rates. • \nTypes of Data: \nBehavioral – What users do (page views, clicks, navigation paths). Attitudinal – Why they do it (motivation, preferences, satisfaction). • \nBenefits: \no Identifies popular content."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "Attitudinal – Why they do it (motivation, preferences, satisfaction). • \nBenefits: \no Identifies popular content. o Helps in customer segmentation. o Aids in ROI measurement for campaigns. • \nChallenges: \no Accuracy affected by bots, caching, cookie deletion. o Data tied to devices, not individuals. 2. Competitive Intelligence \n• \nDefinition: \nThe process of gathering and analyzing data about competitors’ online \nperformance and market trends."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "2. Competitive Intelligence \n• \nDefinition: \nThe process of gathering and analyzing data about competitors’ online \nperformance and market trends. • \nMethods: \no Panel-based data: From users who install tracking software (e.g., \ncomScore). o ISP-based data: From Internet Service Providers (e.g., Hitwise). o Search engine data: Google Trends, Microsoft adCenter. • \nApplications: \no Identify market share trends."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "o Spot new market opportunities. o Understand competitor traffic sources and strategies. • \nBest Practices: \no Segment competitor data by geography, device, or demographic. o Compare over time, not just in single snapshots. 3. Research Data \n• \nPurpose: Understand the “why” behind user actions (qualitative \ninsight). • \nMethods: \no Surveys: Collect feedback on satisfaction, intent, preferences. o Heuristic evaluations: Expert review based on usability \nprinciples."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "• \nMethods: \no Surveys: Collect feedback on satisfaction, intent, preferences. o Heuristic evaluations: Expert review based on usability \nprinciples. o Usability testing: Observe users completing defined tasks. o Site visits: Observe real-world usage contexts. • \nBenefits: \no Reveals hidden pain points. o Complements clickstream (quantitative) data. • \nOrganizational Note: Best results occur when research and analytics \nteams collaborate. 4."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "• \nOrganizational Note: Best results occur when research and analytics \nteams collaborate. 4. Critical Components \n• \nCustomer Centricity: \no Prioritize user needs over purely business-focused metrics. o Measure satisfaction, task completion, content findability. • \nBusiness Questions Approach: \no Shift from “give me a report” to “help me solve X business \nproblem.” \no Use open-ended, action-oriented questions. • \n10/90 Rule:"}
{"source": "IMP Q WA.pdf", "page": 3, "text": "o Spend 10% on tools, 90% on skilled people who can interpret and \nact on data. • \nIdeal Analyst Traits: \no Tool-agnostic, curious, communicative, and business-minded. • \nOrganizational Ownership: \no Analytics should be business-owned, not purely IT-driven. 5. KPIs & Usability Testing \nKPIs (Key Performance Indicators) \n• \nDefinition: Metrics directly linked to business goals. • \nExamples: \no Commerce: Conversion rate, average order value. o Lead gen: Cost per lead, bounce rate."}
{"source": "IMP Q WA.pdf", "page": 3, "text": "• \nExamples: \no Commerce: Conversion rate, average order value. o Lead gen: Cost per lead, bounce rate. o Support: Task completion, search query trends. • \nSelection Process: \no Define site purpose. o Identify main visitor segments. o Choose metrics that indicate success for each segment. Usability Testing \n• \nPurpose: Evaluate how easy and efficient it is for users to complete \ntasks on a website. • \nProcess: Prepare → Conduct → Analyze → Follow-up."}
{"source": "IMP Q WA.pdf", "page": 3, "text": "Usability Testing \n• \nPurpose: Evaluate how easy and efficient it is for users to complete \ntasks on a website. • \nProcess: Prepare → Conduct → Analyze → Follow-up. • \nBenefits: Detects usability issues early, provides context for analytics \ndata. 6. Heuristic Evaluation \n• \nDefinition: Expert review of a site/application using established \nusability heuristics. • \nPrinciples Used (examples): \no Visibility of system status."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "o Match between system and real world. o User control and freedom. o Error prevention and recovery. • \nAdvantages: \no Quick and cost-effective. o Can be performed early in development. • \nLimitations: \no May miss issues only revealed through actual user interaction. o Best when combined with usability testing. 7. Survey Feedback \n• \nPurpose: Collect attitudinal data directly from users. • \nBest Practices: \no Keep surveys short (1–3 questions) to encourage completion."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "Survey Feedback \n• \nPurpose: Collect attitudinal data directly from users. • \nBest Practices: \no Keep surveys short (1–3 questions) to encourage completion. o Ask only actionable questions (e.g., “What prevented you from \npurchasing today?”). o Use cookie-based controls to avoid over-surveying. • \nDelivery Methods: \no On-page popups after task completion. o Email follow-ups for recent visitors. • \nAdvantages: \no Direct voice-of-customer data."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "o Email follow-ups for recent visitors. • \nAdvantages: \no Direct voice-of-customer data. o Identifies satisfaction levels and improvement areas. 8. Types of Data \n1. Behavioral Data – Quantitative, from logs/tags: \no Page views, clicks, time on page, navigation flow. 2. Attitudinal Data – Qualitative, from user feedback: \no Satisfaction levels, reasons for visits, perceived ease of use."}
{"source": "IMP Q WA.pdf", "page": 5, "text": "3. Outcome Data – Business result-oriented: \no Orders placed, leads generated, support cases resolved. 4. Competitive Data – External benchmarking: \no Market share, traffic rank, competitor keywords. 5. Research Data – Contextual understanding: \no Usability tests, heuristic reviews, in-depth interviews."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "1. Understanding Web Analytics \n• \nDefinition: \nMeasurement, collection, analysis, and reporting of internet data for \nunderstanding and optimizing web usage. • \nPurpose: \no Improve website effectiveness. o Conduct market and business research. o Track marketing campaigns and conversion rates. • \nTypes of Data: \nBehavioral – What users do (page views, clicks, navigation paths). Attitudinal – Why they do it (motivation, preferences, satisfaction). • \nBenefits: \no Identifies popular content."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "Attitudinal – Why they do it (motivation, preferences, satisfaction). • \nBenefits: \no Identifies popular content. o Helps in customer segmentation. o Aids in ROI measurement for campaigns. • \nChallenges: \no Accuracy affected by bots, caching, cookie deletion. o Data tied to devices, not individuals. 2. Competitive Intelligence \n• \nDefinition: \nThe process of gathering and analyzing data about competitors’ online \nperformance and market trends."}
{"source": "IMP Q WA.pdf", "page": 1, "text": "2. Competitive Intelligence \n• \nDefinition: \nThe process of gathering and analyzing data about competitors’ online \nperformance and market trends. • \nMethods: \no Panel-based data: From users who install tracking software (e.g., \ncomScore). o ISP-based data: From Internet Service Providers (e.g., Hitwise). o Search engine data: Google Trends, Microsoft adCenter. • \nApplications: \no Identify market share trends."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "o Spot new market opportunities. o Understand competitor traffic sources and strategies. • \nBest Practices: \no Segment competitor data by geography, device, or demographic. o Compare over time, not just in single snapshots. 3. Research Data \n• \nPurpose: Understand the “why” behind user actions (qualitative \ninsight). • \nMethods: \no Surveys: Collect feedback on satisfaction, intent, preferences. o Heuristic evaluations: Expert review based on usability \nprinciples."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "• \nMethods: \no Surveys: Collect feedback on satisfaction, intent, preferences. o Heuristic evaluations: Expert review based on usability \nprinciples. o Usability testing: Observe users completing defined tasks. o Site visits: Observe real-world usage contexts. • \nBenefits: \no Reveals hidden pain points. o Complements clickstream (quantitative) data. • \nOrganizational Note: Best results occur when research and analytics \nteams collaborate. 4."}
{"source": "IMP Q WA.pdf", "page": 2, "text": "• \nOrganizational Note: Best results occur when research and analytics \nteams collaborate. 4. Critical Components \n• \nCustomer Centricity: \no Prioritize user needs over purely business-focused metrics. o Measure satisfaction, task completion, content findability. • \nBusiness Questions Approach: \no Shift from “give me a report” to “help me solve X business \nproblem.” \no Use open-ended, action-oriented questions. • \n10/90 Rule:"}
{"source": "IMP Q WA.pdf", "page": 3, "text": "o Spend 10% on tools, 90% on skilled people who can interpret and \nact on data. • \nIdeal Analyst Traits: \no Tool-agnostic, curious, communicative, and business-minded. • \nOrganizational Ownership: \no Analytics should be business-owned, not purely IT-driven. 5. KPIs & Usability Testing \nKPIs (Key Performance Indicators) \n• \nDefinition: Metrics directly linked to business goals. • \nExamples: \no Commerce: Conversion rate, average order value. o Lead gen: Cost per lead, bounce rate."}
{"source": "IMP Q WA.pdf", "page": 3, "text": "• \nExamples: \no Commerce: Conversion rate, average order value. o Lead gen: Cost per lead, bounce rate. o Support: Task completion, search query trends. • \nSelection Process: \no Define site purpose. o Identify main visitor segments. o Choose metrics that indicate success for each segment. Usability Testing \n• \nPurpose: Evaluate how easy and efficient it is for users to complete \ntasks on a website. • \nProcess: Prepare → Conduct → Analyze → Follow-up."}
{"source": "IMP Q WA.pdf", "page": 3, "text": "Usability Testing \n• \nPurpose: Evaluate how easy and efficient it is for users to complete \ntasks on a website. • \nProcess: Prepare → Conduct → Analyze → Follow-up. • \nBenefits: Detects usability issues early, provides context for analytics \ndata. 6. Heuristic Evaluation \n• \nDefinition: Expert review of a site/application using established \nusability heuristics. • \nPrinciples Used (examples): \no Visibility of system status."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "o Match between system and real world. o User control and freedom. o Error prevention and recovery. • \nAdvantages: \no Quick and cost-effective. o Can be performed early in development. • \nLimitations: \no May miss issues only revealed through actual user interaction. o Best when combined with usability testing. 7. Survey Feedback \n• \nPurpose: Collect attitudinal data directly from users. • \nBest Practices: \no Keep surveys short (1–3 questions) to encourage completion."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "Survey Feedback \n• \nPurpose: Collect attitudinal data directly from users. • \nBest Practices: \no Keep surveys short (1–3 questions) to encourage completion. o Ask only actionable questions (e.g., “What prevented you from \npurchasing today?”). o Use cookie-based controls to avoid over-surveying. • \nDelivery Methods: \no On-page popups after task completion. o Email follow-ups for recent visitors. • \nAdvantages: \no Direct voice-of-customer data."}
{"source": "IMP Q WA.pdf", "page": 4, "text": "o Email follow-ups for recent visitors. • \nAdvantages: \no Direct voice-of-customer data. o Identifies satisfaction levels and improvement areas. 8. Types of Data \n1. Behavioral Data – Quantitative, from logs/tags: \no Page views, clicks, time on page, navigation flow. 2. Attitudinal Data – Qualitative, from user feedback: \no Satisfaction levels, reasons for visits, perceived ease of use."}
{"source": "IMP Q WA.pdf", "page": 5, "text": "3. Outcome Data – Business result-oriented: \no Orders placed, leads generated, support cases resolved. 4. Competitive Data – External benchmarking: \no Market share, traffic rank, competitor keywords. 5. Research Data – Contextual understanding: \no Usability tests, heuristic reviews, in-depth interviews."}
